{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8b9d7",
   "metadata": {},
   "source": [
    "## Colour Correction and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aede22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColourCorrection(image_path: str = None) -> Optional[np.array]:\n",
    "    \"\"\"\n",
    "    Performs colour correction and normalization on input image.\n",
    "\n",
    "    args:\n",
    "    - image_path (str): path to input image\n",
    "\n",
    "    returns:\n",
    "    - corrected and normalized image data (as numpy array)\n",
    "    \"\"\"\n",
    "    if not image_path:\n",
    "        return None\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # brightness normalization by gamma correction\n",
    "    gamma = 1.0     # to be adjusted by passing gamma as parameter to function?\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 \n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    \n",
    "    img_gamma = cv2.LUT(img, table)\n",
    "\n",
    "    # image values normalized to between 0 and 1\n",
    "    img_normalized = cv2.normalize(img_gamma, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    \n",
    "    return img_normalized\n",
    "\n",
    "    \"\"\"\n",
    "    Gamma correction code referenced from:\n",
    "    https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c552e",
   "metadata": {},
   "source": [
    "## Patching Module\n",
    "Turns palm image into sequence of fixed-size patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects sequence of patches of palm / palm regions into low-dimensional space to produce a patch embedding sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 768):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the Patching module.\n",
    "\n",
    "        args:\n",
    "        - image_size (int): size of image\n",
    "        - patch_size (int): size of patches\n",
    "        - in_channels (int): number of input channels (default: 3 for RGB)\n",
    "        - embed_dim (int): dimension of output vector embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # input image: C x H x W\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=embed_dim, \n",
    "                      kernel_size=patch_size, \n",
    "                      stride=patch_size),\n",
    "            # after projection as convolution: embed_dim x num of patches (H) x num of patches (W)\n",
    "            nn.Flatten(1,2),\n",
    "            # embed_dim x num of patches in total\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for input image and converts image into patch embeddings.\n",
    "\n",
    "        args:\n",
    "        - x (torch.Tensor): input image (C, H, W)\n",
    "\n",
    "        returns:\n",
    "        - embedded patches of the image\n",
    "        \"\"\"\n",
    "        x = self.model(x)           # embed_dim x num of patches in total\n",
    "        x = x.transpose(0,1)        # num of patches in total x embed_dim\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db605b05",
   "metadata": {},
   "source": [
    "## Positional Embedding Module\n",
    "Adds positional embedding to sequence of patch embeddings\n",
    "\n",
    "Positional embedding formula:\n",
    "$$PE_{(\\text{pos}, 2i)} = \\sin (\\dfrac{\\text{pos}}{10000^{2i / d_{\\text{model}}}})$$\n",
    "$$PE_{(\\text{pos}, 2i+1)} = \\cos (\\dfrac{\\text{pos}}{10000^{2i / d_{\\text{model}}}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Provides positional embedding for patches to identify location of patch in original image.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the PositionalEmbedding module.\n",
    "\n",
    "        args:\n",
    "        - d_model (int): dimensions of embedding\n",
    "        - max_len (int): maximum length of sequence\n",
    "        \"\"\"\n",
    "        super.__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        denominator_terms = torch.pow(10000, torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position / denominator_terms)\n",
    "        pe[:, 1::2] = torch.cos(position / denominator_terms)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional embedding to an input sequence.\n",
    "\n",
    "        args:\n",
    "        - x (torch.Tensor): input sequence\n",
    "\n",
    "        returns:\n",
    "        - input sequence with positional embedding added\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size[1], :]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
