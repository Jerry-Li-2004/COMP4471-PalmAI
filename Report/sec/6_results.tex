\section{Experiments}
\label{sec:results}

\subsection{Vision Transformer for Palm Image Regression}

\subsubsection{Architecture Overview}
The Vision Transformer (ViT) processes palm images for regression of four scores: strength, romantic, luck, and potential. Key components:

\begin{itemize}
    \item \textbf{Backbone}: Pre-trained ViT-Base (12 layers, 768-dim, patch size 16)
    \item \textbf{Input}: $224 \times 224$ RGB â†’ 196 patches
    \item \textbf{Regression head}: 3-layer MLP with GELU, dropout, and sigmoid output
    \item \textbf{Output}: Four scores in $[0,1]$ range
\end{itemize}

\subsubsection{Training Configuration}
\begin{center}
  \begin{tabular}{ll}
    \toprule
    Parameter & Value \\
    \midrule
    Batch size & 8 \\
    Learning rate & $2 \times 10^{-4}$ \\
    Epochs & 80 \\
    Warmup & 5\% (fast) \\
    Optimizer & AdamW (wd=0.01) \\
    Loss & MSE \\
    Scheduler & OneCycleLR \\
    \bottomrule
  \end{tabular}
\end{center}

\subsubsection{Data Processing}
\begin{itemize}
    \item \textbf{Augmentation}: Random crop, flip, rotation ($\pm 15^\circ$), color jitter
    \item \textbf{Normalization}: Mean = 0.5, std = 0.5
    \item \textbf{Labels}: JSON parsing with error handling
\end{itemize}

\subsubsection{Training Results}
The model achieved strong performance in palm score regression:

\begin{itemize}
    \item \textbf{Best model at epoch 45} (early convergence)
    \item \textbf{Validation loss}: 0.0186 MSE
    \item \textbf{Validation MAE}: 0.1002 (average absolute error)
    \item \textbf{Practical implication}: Predictions within $\pm$0.1 of true values
\end{itemize}

\subsubsection{Training Curves}
Training performance is visualized through loss curves showing model convergence:

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Images/vis_training_curves_up.png}
\caption{Training curves showing convergence at epoch 45. The model achieved validation loss 0.0186 and MAE 0.1002, indicating effective learning of palm features.}
\label{fig:training_curves}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{Images/vis_training_curves_down.png}
\caption{Root Mean Square Error (RMSE) curves during training. The model's RMSE decreased steadily till epoch 45.}
\label{fig:training_curves}
\end{figure}

\subsubsection{Key Advantages}
\begin{itemize}
    \item \textbf{Global context}: Self-attention captures relationships across entire palm
    \item \textbf{Transfer learning}: ImageNet-21k pre-training provided strong initialization
    \item \textbf{Fast convergence}: Best performance reached at epoch 45 of 80
    \item \textbf{High accuracy}: MAE of 0.1002 indicates precise score prediction
\end{itemize}

\subsection{Alternative Framework Using CNNs}

\subsubsection{Training Configuration}
For all CNN architectures trained and tested, the parameters are the same and are listed in the following table.
\begin{center}
  \begin{tabular}{ll}
    \toprule
    Parameter & Value \\
    \midrule
    Batch size & 8 \\
    Learning rate & $1 \times 10^{-4}$ \\
    Dropout rate & 0.2 or 0.3 \\
    Epochs & 20 \\
    Warmup & 5\% (fast) \\
    Optimizer & AdamW (wd=0.01) \\
    Loss & MSE \\
    Scheduler & ReduceLROnPlateau \\
    \bottomrule
  \end{tabular}
\end{center}

\subsubsection{Multi-scale Feature Extraction CNN}
This architecture performs multiple convolutions to extract features at different levels,
then passes the features through an attention mechanism to focus on important palm line regions,
and finally fuses the feature outputs to regress to 4 scores.
The detailed flow is as follows.
\begin{itemize}
  \item Initial convolution: 1 channel to 32 channels, kernel size 3, padding 1
  \item Mid-level features: 32 channels to 64 channels, kernel size 3, padding 1
  \item High-level features: 64 channels to 128 channels, kernel size 3, padding 1
  \item Attention (from high-level features): 2D Conv from 256 channels to 1 channel, kernel size 1, padding 0
  \item Global features (from attended features): 128 channels to 256 channls, kernel size 3, padding 1
  \item Further upscaling (from high-level features): 128 channels to 256 channls, kernel size 3, padding 1
  \item Feature fusion: combines further upscaling features and global features
  \item Regression to 4 scores: linear layers
  \item ReLU layer to clip values between 0 and 1
\end{itemize}
This architecture gave an average MSE loss of 0.0168 on evaluation using 800 images.
The final training loss was 0.0096, and the best validation loss was 0.0231.

\subsubsection{Transfer Learning CNN}
This architecture uses a pretrained backbone, either EfficientNet or ResNet, to extract features.
The extracted features are then flattened and passed through linear layers,
and finally a regression layer to output to 4 scores.
A sigmoid activation is used before outputting the scores to clip scores between 0 and 1.

This architecture gave an average MSE loss of 0.0075 on evaluation using 800 images, the best among all CNN architectures tested.
The final training loss was 0.0063, and the best validation loss was 0.0251.

Since our dataset is rather small and only consists of 1600 images, 
we find transfer learning to be the most efficient and provides the best results.

\subsubsection{Region-specialized CNN}
This architecture uses a shared backbone to first extract features,
then passes the features through 4 parallel branches, each attending to a different palm region,
and finally concatenates the features from the branches to regress to 4 scores.
The detailed flow is as follows.
\begin{itemize}
  \item Shared backbone: 2D convolutionse to 32 then 64 and 128 channels, kernel size 3, padding 1
  \item Parallel branches:
  \begin{itemize}
    \item Attention in each branch: 2D convolution from 128 to 64 then 32 channels, kernel size 1, padding 0
    \item Region extractor in each branch: 2D convolution from 128 to 64 channels, kernel size 3, padding 1
  \end{itemize}
  \item Feature concatenation: linear layer from 64 $\times$ 4 dimensions to 256 dimensions
  \item Regression to scores: linear layers from 256 dimensions to 128 then 4 dimensions
  \item ReLU layer to clip values between 0 and 1
\end{itemize}
This architecture gave an average MSE loss of 0.0304 on evaluation using 800 images.
The final training loss was 0.0191, and the best validation loss was 0.0237.

We hypothesize that each palm region has different semantic meanings, hence each gives one score per aspect of life.
Thus in this architecture, we use parallel convolution operations on each region to extract features and translate those features to scores.

\subsubsection{Summary of CNN Training}
\begin{center}
  \begin{tabular}{llll}
    \hline
    Architecture & Avg. Loss & Train Loss & Val. Loss \\
    \hline
    Multi-scale & 0.0168 & 0.0096 & 0.0231 \\
    Transfer Learning & 0.0075 & 0.0063 & 0.0251 \\
    Region-specialized & 0.0304 & 0.0191 & 0.0237 \\
    \hline
  \end{tabular}
\end{center}

\subsection{User Interface}
We provide a graphical user interface for seamless use of our pipeline, through a web app.
The user first scans their palm using an on-device camera, and will receive predictions in natural language with an LLM.
The user may ask further questions and through a chat-like dialogue.

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{Images/ui_main.png}

   \caption{Homepage of our web app.}
   \label{fig:onecol}
\end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{Images/ui_palm.png}

   \caption{User interface prompting for palm scanning.}
   \label{fig:onecol}
\end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{Images/ui_chat.png}

   \caption{Follow-up dialogue with LLM.}
   \label{fig:onecol}
\end{figure}
