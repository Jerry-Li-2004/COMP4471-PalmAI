\section{Technical Approach and Model Architecture}
\label{sec:technical}

\begin{figure}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{Images/pipeline.png}

   \caption{Our proposed pipeline structure.}
   \label{fig:onecol}
\end{figure}

Our system is designed as a novel, multi-modal pipeline that integrates computer vision and natural language processing to translate palm features into insightful descriptions. 
The architecture moves beyond a simple feature-to-text model by incorporating a rule-based scoring mechanism and a final Large Language Model (LLM) synthesis step for coherent and contextualized output.


\subsection{Feature Extraction}

\subsubsection{Preprocessing}
The pipeline begins by preprocessing the raw palm image to standardize input and highlight regions of interest. 
This involves:

\begin{itemize}
    \item \textbf{Color Correction \& Normalization}: Adjusting for lighting variations to ensure consistent color and contrast across images.
    \item \textbf{Pale Region Isolation}: A specialized segmentation model (e.g. a lightweight U-Net) is employed to accurately identify and crop the four primary pale regions corresponding to the mounts of the palm (e.g., Mount of Venus, Mount of Jupiter, etc.), as well as the central palmar area containing the primary lines (Heart, Head, Life).
    \item \textbf{Image Patching}: The isolated full palm and/or specific regions are divided into a sequence of fixed-size patches, which are then linearly projected into a lower-dimensional space to create a patch embedding sequence.
\end{itemize}

\subsubsection{Feature Extraction with Hybrid Transformer}
Instead of a single transformer, we propose a Hybrid Transformer Encoder architecture with the following features, as shown in Figure 6.

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{Images/hybrid-transformer-1.jpeg}

   \caption{Feature Extraction with Hybrid Transformer.}
   \label{fig:onecol}
\end{figure}

\begin{itemize}
    \item \textbf{Shared Backbone}: A Vision Transformer (ViT) backbone first processes the entire preprocessed palm image to generate a global context embedding, represented by the [CLS] token.
    \item \textbf{Region-Specific Encoders}: In parallel, the cropped image regions containing specific palm lines (Heart, Head, Life, Fate) are processed by smaller, independent transformer encoders. This allows for focused feature extraction tailored to the unique characteristics of each line.
    \item \textbf{Feature Fusion}: The global [CLS] token embedding and the features from the region-specific encoders are concatenated and passed through a fusion module (e.g., a cross-attention layer or a simple fully connected layer) to create a unified, comprehensive feature representation $F_{\text{unified}}$.
\end{itemize}


\subsection{Feature Processing and Multi-Modal Interpretation}
This stage interprets the fused features $F_{\text{unified}}$ through two parallel paths.

\subsubsection{Text Decoder}
A pre-trained text decoder (e.g., a Transformer Decoder or a GPT-2 architecture) takes $F_{\text{unified}}$ as input. 
It is trained to generate structured, factual descriptions in natural language for each palm line in a template-like format.
Our template is defined as the following.

\begin{itemize}
    \item \textbf{Input}: $F_{\text{unified}}$ (Feature vector for a specific palm).
    \item \textbf{Expected Output Format}: "The heart line is long and curvy. The head line is deep and long. The life line curves completely around the thumb."
\end{itemize}

\subsubsection{Embeddings and Score Prediction}
This branch translates the image features into a quantitative and comparable format.

\begin{itemize}
    \item \textbf{Embedding Generation}: The feature vector  $F_{\text{unified}}$ is projected into a lower-dimensional, normalized embedding space (e.g., using a dedicated Embedding Layer). The core hypothesis is that palms with similar palmistry traits will lie close to each other in this space (high cosine similarity). We train our model using contrastive methods, by minimizing the dot product between embeddings of palms with similar traits, and maximizing that for embeddings of palms with traits that differ more.
    \item \textbf{Rule-Based Scoring Engine}: This is a novel, explicit component. A set of deterministic, human-defined rules based on traditional palmistry (e.g., "if the life line is curved, add 5 points to vitality score") is applied directly to the structured descriptions generated by the Text Decoder branch. This generates a score vector $S_{\text{rule}}$ (e.g. [Vitality: 8, Creativity: 5, Logic: 3]).
    \item \textbf{Neural Network Score Predictor}: In parallel, a shallow Neural Network (NN) processes the unified embedding to predict a complementary score vector $S_{\text{NN}}$. The model is trained so that $S_{\text{NN}}$ correlates with the rule-based scores $S_{\text{rule}}$ while also learning subtle, non-linear patterns from the data.
\end{itemize}


\subsection{LLM Inference for Holistic Synthesis}
The final stage leverages a Large Language Model (LLM) as a sophisticated information synthesizer.

\begin{itemize}
    \item \textbf{Input Prompt Construction}: The outputs from previous stages are assembled into a detailed prompt for the LLM. We include structured descriptions from the text decoder, score vectors $S_{\text{NN}}$ and $S_{\text{rule}}$, as well as a system prompt framing the task.
    \item \textbf{Natural Language Generation}: The LLM (e.g., a fine-tuned Llama 2 or GPT-3.5 model) processes this rich, multi-modal input to produce the final output. It does not just paraphrase; it reasons about the interplay between different lines and scores, generating a novel, insightful, and human-readable report that includes character analysis and life suggestions. This step adds a layer of interpretation and narrative fluency that is absent in simpler sequence-to-sequence models.
\end{itemize}

\begin{figure}[t]
    \begin{verbatim}
You are a palmistry expert. 
You will assist the user in inferring 
fortune-telling results based on their 
palm line features.

## Supplementary Information on Palmistry
Palm lines of interest include the *Life 
Line*, the *Heart Line*, the *Fate Line*, 
and the *Head Line*. These lines 
represent respectively, *enthusiasm 
and strength*, *romantic life*, *fortune 
and luck*, and *smartness and potential*. 
You will receive scores predicted based 
on palm line features by a Deep Learning 
model. You are then to provide fortune-
telling results in natural language, 
further instructions will be given in 
the next section.

### About Each Palm Line
<supporting information>

## Inference Instructions
You will be provided with scores for 
`strength`, `romantic`, `luck`, and 
`potential`. These scores were previously 
predicted using a deep learning model 
based on geometric features of the user's 
palm lines, and they correspond to the 
above points provided to you as 
supplementary information on palmistry.

Provide the user with fortune-telling 
results based on these scores, and give 
explanations to assist the user in 
inferring their results. Explain what 
those scores mean and how their luck 
or fortune looks like. Give advice to 
the user on how to face probable events 
in their future life.
    \end{verbatim}
  \centering

   \caption{Contextual instructions provided through the system prompt to the LLM.}
   \label{fig:onecol}
\end{figure}